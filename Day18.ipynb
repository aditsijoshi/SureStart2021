{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Day18.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPxZN8OvQkdNVMB+QF9Kr6S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aditsijoshi/SureStart2021/blob/main/Day18.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWAUSd1pHl3B"
      },
      "source": [
        "# Basic packages\r\n",
        "import pandas as pd \r\n",
        "import numpy as np\r\n",
        "import re\r\n",
        "import collections\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from pathlib import Path\r\n",
        "# Packages for data preparation\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from keras.preprocessing.text import Tokenizer\r\n",
        "from keras.utils.np_utils import to_categorical\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "# Packages for modeling\r\n",
        "from keras import models\r\n",
        "from keras import layers\r\n",
        "from keras import regularizers\r\n",
        "\r\n",
        "NB_WORDS = 10000  # Parameter indicating the number of words we'll put in the dictionary\r\n",
        "NB_START_EPOCHS = 20  # Number of epochs we usually start to train with\r\n",
        "BATCH_SIZE = 512  # Size of the batches used in the mini-batch gradient descent\r\n",
        "MAX_LEN = 20  # Maximum number of words in a sequence"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKdqGUCPHuah"
      },
      "source": [
        "def deep_model(model, X_train, y_train, X_valid, y_valid):\r\n",
        "    '''\r\n",
        "    Function to train a multi-class model. The number of epochs and \r\n",
        "    batch_size are set by the constants at the top of the\r\n",
        "    notebook. \r\n",
        "    \r\n",
        "    Parameters:\r\n",
        "        model : model with the chosen architecture\r\n",
        "        X_train : training features\r\n",
        "        y_train : training target\r\n",
        "        X_valid : validation features\r\n",
        "        Y_valid : validation target\r\n",
        "    Output:\r\n",
        "        model training history\r\n",
        "    '''\r\n",
        "    model.compile(optimizer='rmsprop'\r\n",
        "                  , loss='categorical_crossentropy'\r\n",
        "                  , metrics=['accuracy'])\r\n",
        "    \r\n",
        "    history = model.fit(X_train\r\n",
        "                       , y_train\r\n",
        "                       , epochs=NB_START_EPOCHS\r\n",
        "                       , batch_size=BATCH_SIZE\r\n",
        "                       , validation_data=(X_valid, y_valid)\r\n",
        "                       , verbose=0)\r\n",
        "    return history\r\n",
        "def eval_metric(model, history, metric_name):\r\n",
        "    '''\r\n",
        "    Function to evaluate a trained model on a chosen metric. \r\n",
        "    Training and validation metric are plotted in a\r\n",
        "    line chart for each epoch.\r\n",
        "    \r\n",
        "    Parameters:\r\n",
        "        history : model training history\r\n",
        "        metric_name : loss or accuracy\r\n",
        "    Output:\r\n",
        "        line chart with epochs of x-axis and metric on\r\n",
        "        y-axis\r\n",
        "    '''\r\n",
        "    metric = history.history[metric_name]\r\n",
        "    val_metric = history.history['val_' + metric_name]\r\n",
        "    e = range(1, NB_START_EPOCHS + 1)\r\n",
        "    plt.plot(e, metric, 'bo', label='Train ' + metric_name)\r\n",
        "    plt.plot(e, val_metric, 'b', label='Validation ' + metric_name)\r\n",
        "    plt.xlabel('Epoch number')\r\n",
        "    plt.ylabel(metric_name)\r\n",
        "    plt.title('Comparing training and validation ' + metric_name + ' for ' + model.name)\r\n",
        "    plt.legend()\r\n",
        "    plt.show()\r\n",
        "def test_model(model, X_train, y_train, X_test, y_test, epoch_stop):\r\n",
        "    '''\r\n",
        "    Function to test the model on new data after training it\r\n",
        "    on the full training data with the optimal number of epochs.\r\n",
        "    \r\n",
        "    Parameters:\r\n",
        "        model : trained model\r\n",
        "        X_train : training features\r\n",
        "        y_train : training target\r\n",
        "        X_test : test features\r\n",
        "        y_test : test target\r\n",
        "        epochs : optimal number of epochs\r\n",
        "    Output:\r\n",
        "        test accuracy and test loss\r\n",
        "    '''\r\n",
        "    model.fit(X_train\r\n",
        "              , y_train\r\n",
        "              , epochs=epoch_stop\r\n",
        "              , batch_size=BATCH_SIZE\r\n",
        "              , verbose=0)\r\n",
        "    results = model.evaluate(X_test, y_test)\r\n",
        "    print()\r\n",
        "    print('Test accuracy: {0:.2f}%'.format(results[1]*100))\r\n",
        "    return results\r\n",
        "    \r\n",
        "def remove_stopwords(input_text):\r\n",
        "    '''\r\n",
        "    Function to remove English stopwords from a Pandas Series.\r\n",
        "    \r\n",
        "    Parameters:\r\n",
        "        input_text : text to clean\r\n",
        "    Output:\r\n",
        "        cleaned Pandas Series \r\n",
        "    '''\r\n",
        "    stopwords_list = stopwords.words('english')\r\n",
        "    # Some words which might indicate a certain sentiment are kept via a whitelist\r\n",
        "    whitelist = [\"n't\", \"not\", \"no\"]\r\n",
        "    words = input_text.split() \r\n",
        "    clean_words = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) > 1] \r\n",
        "    return \" \".join(clean_words) \r\n",
        "    \r\n",
        "def remove_mentions(input_text):\r\n",
        "    '''\r\n",
        "    Function to remove mentions, preceded by @, in a Pandas Series\r\n",
        "    \r\n",
        "    Parameters:\r\n",
        "        input_text : text to clean\r\n",
        "    Output:\r\n",
        "        cleaned Pandas Series \r\n",
        "    '''\r\n",
        "    return re.sub(r'@\\w+', '', input_text)\r\n",
        "def compare_models_by_metric(model_1, model_2, model_hist_1, model_hist_2, metric):\r\n",
        "    '''\r\n",
        "    Function to compare a metric between two models \r\n",
        "    \r\n",
        "    Parameters:\r\n",
        "        model_hist_1 : training history of model 1\r\n",
        "        model_hist_2 : training history of model 2\r\n",
        "        metrix : metric to compare, loss, acc, val_loss or val_acc\r\n",
        "        \r\n",
        "    Output:\r\n",
        "        plot of metrics of both models\r\n",
        "    '''\r\n",
        "    metric_model_1 = model_hist_1.history[metric]\r\n",
        "    metric_model_2 = model_hist_2.history[metric]\r\n",
        "    e = range(1, NB_START_EPOCHS + 1)\r\n",
        "    \r\n",
        "    metrics_dict = {\r\n",
        "        'acc' : 'Training Accuracy',\r\n",
        "        'loss' : 'Training Loss',\r\n",
        "        'val_acc' : 'Validation accuracy',\r\n",
        "        'val_loss' : 'Validation loss'\r\n",
        "    }\r\n",
        "    \r\n",
        "    metric_label = metrics_dict[metric]\r\n",
        "    plt.plot(e, metric_model_1, 'bo', label=model_1.name)\r\n",
        "    plt.plot(e, metric_model_2, 'b', label=model_2.name)\r\n",
        "    plt.xlabel('Epoch number')\r\n",
        "    plt.ylabel(metric_label)\r\n",
        "    plt.title('Comparing ' + metric_label + ' between models')\r\n",
        "    plt.legend()\r\n",
        "    plt.show()\r\n",
        "    \r\n",
        "def optimal_epoch(model_hist):\r\n",
        "    '''\r\n",
        "    Function to return the epoch number where the validation loss is\r\n",
        "    at its minimum\r\n",
        "    \r\n",
        "    Parameters:\r\n",
        "        model_hist : training history of model\r\n",
        "    Output:\r\n",
        "        epoch number with minimum validation loss\r\n",
        "    '''\r\n",
        "    min_epoch = np.argmin(model_hist.history['val_loss']) + 1\r\n",
        "    print(\"Minimum validation loss reached in epoch {}\".format(min_epoch))\r\n",
        "    return min_epoch"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "eDSyZrBfHzO2",
        "outputId": "3d1aba49-aaa0-43cb-f1a3-4e3b9f3d1f4a"
      },
      "source": [
        "df = pd.read_csv('/content/Tweets.csv')\r\n",
        "df = df.reindex(np.random.permutation(df.index))  \r\n",
        "df = df[['text', 'airline_sentiment']]\r\n",
        "df.text = df.text.apply(remove_stopwords).apply(remove_mentions)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/lib/nltk_data'\n**********************************************************************\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-ad5709f43a80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'airline_sentiment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremove_stopwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremove_mentions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   4211\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4212\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4213\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4215\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-b4166f0a2d31>\u001b[0m in \u001b[0;36mremove_stopwords\u001b[0;34m(input_text)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mcleaned\u001b[0m \u001b[0mPandas\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     '''\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0mstopwords_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0;31m# Some words which might indicate a certain sentiment are kept via a whitelist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0mwhitelist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"n't\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"not\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"no\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "376k_gbNH4G4"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(df.text, df.airline_sentiment, test_size=0.1, random_state=37)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuYRnD8GH8kL"
      },
      "source": [
        "tk = Tokenizer(num_words=NB_WORDS,\r\n",
        "               filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{\"}~\\t\\n',\r\n",
        "               lower=True,\r\n",
        "               char_level=False,\r\n",
        "               split=' ')\r\n",
        "tk.fit_on_texts(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTWc8MDuH_CF"
      },
      "source": [
        "X_train_oh = tk.texts_to_matrix(X_train, mode='binary')\r\n",
        "X_test_oh = tk.texts_to_matrix(X_test, mode='binary')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tj8UHdxwIBOS"
      },
      "source": [
        "le = LabelEncoder()\r\n",
        "y_train_le = le.fit_transform(y_train)\r\n",
        "y_test_le = le.transform(y_test)\r\n",
        "y_train_oh = to_categorical(y_train_le)\r\n",
        "y_test_oh = to_categorical(y_test_le)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mta_YSvlIDZC"
      },
      "source": [
        "X_train_rest, X_valid, y_train_rest, y_valid = train_test_split(X_train_oh, y_train_oh, test_size=0.1, random_state=37)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ToESAr4mIFzn"
      },
      "source": [
        "base_model = models.Sequential()\r\n",
        "base_model.add(layers.Dense(64, activation='relu', input_shape=(NB_WORDS,)))\r\n",
        "base_model.add(layers.Dense(64, activation='relu'))\r\n",
        "base_model.add(layers.Dense(3, activation='softmax'))\r\n",
        "base_model.name = 'Baseline model'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDnTX6DsIISM"
      },
      "source": [
        "base_history = deep_model(base_model, X_train_rest, y_train_rest, X_valid, y_valid)\r\n",
        "base_min = optimal_epoch(base_history)\r\n",
        "eval_metric(base_model, base_history, 'loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InyVCsxBILdo"
      },
      "source": [
        "reduced_model = models.Sequential()\r\n",
        "reduced_model.add(layers.Dense(16, activation='relu', input_shape=(NB_WORDS,)))\r\n",
        "reduced_model.add(layers.Dense(3, activation='softmax'))\r\n",
        "reduced_model.name = 'Reduced model'\r\n",
        "reduced_history = deep_model(reduced_model, X_train_rest, y_train_rest, X_valid, y_valid)\r\n",
        "reduced_min = optimal_epoch(reduced_history)\r\n",
        "eval_metric(reduced_model, reduced_history, 'loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDbepzCTIOWK"
      },
      "source": [
        "reg_model = models.Sequential()\r\n",
        "reg_model.add(layers.Dense(64, kernel_regularizer=regularizers.l2(0.001), activation='relu', input_shape=(NB_WORDS,)))\r\n",
        "reg_model.add(layers.Dense(64, kernel_regularizer=regularizers.l2(0.001), activation='relu'))\r\n",
        "reg_model.add(layers.Dense(3, activation='softmax'))\r\n",
        "reg_model.name = 'L2 Regularization model'\r\n",
        "reg_history = deep_model(reg_model, X_train_rest, y_train_rest, X_valid, y_valid)\r\n",
        "reg_min = optimal_epoch(reg_history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoZdrM4_IR3f"
      },
      "source": [
        "eval_metric(reg_model, reg_history, 'loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xoWXmP_IWqZ"
      },
      "source": [
        "compare_models_by_metric(base_model, reg_model, base_history, reg_history, 'val_loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOx4-g3TIbuP"
      },
      "source": [
        "drop_model = models.Sequential()\r\n",
        "drop_model.add(layers.Dense(64, activation='relu', input_shape=(NB_WORDS,)))\r\n",
        "drop_model.add(layers.Dropout(0.5))\r\n",
        "drop_model.add(layers.Dense(64, activation='relu'))\r\n",
        "drop_model.add(layers.Dropout(0.5))\r\n",
        "drop_model.add(layers.Dense(3, activation='softmax'))\r\n",
        "drop_model.name = 'Dropout layers model'\r\n",
        "drop_history = deep_model(drop_model, X_train_rest, y_train_rest, X_valid, y_valid)\r\n",
        "drop_min = optimal_epoch(drop_history)\r\n",
        "eval_metric(drop_model, drop_history, 'loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ckx6MzwTIhW7"
      },
      "source": [
        "compare_models_by_metric(base_model, drop_model, base_history, drop_history, 'val_loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0ZfLcxRImVx"
      },
      "source": [
        "base_results = test_model(base_model, X_train_oh, y_train_oh, X_test_oh, y_test_oh, base_min)\r\n",
        "reduced_results = test_model(reduced_model, X_train_oh, y_train_oh, X_test_oh, y_test_oh, reduced_min)\r\n",
        "reg_results = test_model(reg_model, X_train_oh, y_train_oh, X_test_oh, y_test_oh, reg_min)\r\n",
        "drop_results = test_model(drop_model, X_train_oh, y_train_oh, X_test_oh, y_test_oh, drop_min)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}